# -*- coding: utf-8 -*-
"""deepfake_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LEbZGnUe6dHppJJAeLwxiCD9dv-j8NPU
"""

import os
import torch
import torch.nn as nn
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import torch.optim as optim

VIDEO_DIR_REAL = '/content/drive/MyDrive/data/Celeb-real'
VIDEO_DIR_FAKE = '/content/drive/MyDrive/data/Celeb-synthesis'
OUTPUT_DIR = 'data'
FRAME_INTERVAL = 30

def extract_frames(video_path, output_folder, frame_interval):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Failed to open {video_path}")
        return

# ipython-input-3-d358699ac5a5
import os
import torch
import torch.nn as nn
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import torch.optim as optim
# You are missing the import for cv2, which is used later.
# Add this import statement:
import cv2

# ipython-input-7-d358699ac5a5
VIDEO_DIR_REAL = '/content/drive/MyDrive/data/Celeb-real'
VIDEO_DIR_FAKE = '/content/drive/MyDrive/data/Celeb-synthesis'
OUTPUT_DIR = 'data'
FRAME_INTERVAL = 30

# Combine the parts of extract_frames and add process_videos and the main block
# into one cell after the imports and variable definitions.

# Combine ipython-input-8-d358699ac5a5 and ipython-input-11-d358699ac5a5
# into a single cell.
def extract_frames(video_path, output_folder, frame_interval):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Failed to open {video_path}")
        return

    video_name = os.path.splitext(os.path.basename(video_path))[0]
    frame_count = 0
    saved_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            frame_filename = os.path.join(output_folder, f"{video_name}_frame{frame_count}.jpg")
            cv2.imwrite(frame_filename, frame)
            saved_count += 1

        frame_count += 1

    cap.release()
    print(f"Extracted {saved_count} frames from {video_name}")

def process_videos(video_dir, output_class_dir):
    os.makedirs(output_class_dir, exist_ok=True)
    for file in os.listdir(video_dir):
        if file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
            extract_frames(os.path.join(video_dir, file), output_class_dir, FRAME_INTERVAL)

if __name__ == "__main__":
    # Create output directories
    os.makedirs(os.path.join(OUTPUT_DIR, 'real'), exist_ok=True)
    os.makedirs(os.path.join(OUTPUT_DIR, 'fake'), exist_ok=True)

    # Process real and fake video folders
    process_videos(VIDEO_DIR_REAL, os.path.join(OUTPUT_DIR, 'real'))
    process_videos(VIDEO_DIR_FAKE, os.path.join(OUTPUT_DIR, 'fake'))

    print("✅ Frame extraction complete. Use the 'data/' folder to train your model.")

DATA_DIR = 'data'               # Folder created by extract_frames.py
SAVE_PATH = 'deepfake_model.pt' # Output model path
BATCH_SIZE = 32
EPOCHS = 5                      # Increase for better performance
LEARNING_RATE = 1e-4

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

dataset = datasets.ImageFolder(DATA_DIR, transform=transform)
train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes: real, fake
model = model.to(device)

import torch
import torch.nn as nn
from torchvision import models

# Define the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define and move the model to the device
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes: real, fake
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

print("✅ Training started...")
for epoch in range(EPOCHS):
    model.train()
    running_loss = 0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    acc = correct / total
    print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {running_loss:.4f} | Accuracy: {acc:.4f}")

torch.save(model.state_dict(), SAVE_PATH)
print(f"✅ Model saved to '{SAVE_PATH}'")

